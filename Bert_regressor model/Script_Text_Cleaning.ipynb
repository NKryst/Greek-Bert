{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  essay_id                                          full_text  score\n",
       " 0  000d118  Many people have car where they live. The thin...      3\n",
       " 1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       " 2  001ab80  People always wish they had the same technolog...      4\n",
       " 3  001bdc0  We all heard about Venus, the planet without a...      4\n",
       " 4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3,\n",
       " Index(['essay_id', 'full_text', 'score'], dtype='object'),\n",
       "   essay_id                                          full_text\n",
       " 0  000d118  Many people have car where they live. The thin...\n",
       " 1  000fe60  I am a scientist at NASA that is discussing th...\n",
       " 2  001ab80  People always wish they had the same technolog...,\n",
       " Index(['essay_id', 'full_text'], dtype='object'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries and redefine file paths\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "#  Preprocessing and Text Normalization\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "train_file_path = r'C:\\Users\\nickr\\OneDrive\\Υπολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\Bert_regressor model\\\\learning-agency-lab-automated-essay-scoring-2\\\\train.csv'\n",
    "test_file_path = r'C:\\Users\\nickr\\OneDrive\\Υπολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\Bert_regressor model\\\\learning-agency-lab-automated-essay-scoring-2\\\\test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Display the first few rows of each dataframe and their structure\n",
    "train_data.head(), train_data.columns, test_data.head(), test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Text Normalization\n",
    "* Cleaning Text: Remove or normalize text artifacts like punctuation, capitalization, and special characters that might not contribute to essay scoring.\n",
    "* Tokenization and Lemmatization: Break down text into tokens (words or phrases) and reduce them to their base or dictionary form.\n",
    "* Stopword Removal: Consider the impact of removing common words that may not contribute to the overall meaning of the essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\nickr\\OneDrive\\Υπολο\n",
      "[nltk_data]     γιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\\\...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\nickr\\OneDrive\\Υ\n",
      "[nltk_data]     πολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\\\...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\nickr\\OneDrive\\Υπο\n",
      "[nltk_data]     λογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\\\...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up NLTK with local resources\n",
    "nltk.data.path.append(r'C:\\Users\\nickr\\OneDrive\\Υπολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Specifying a custom path for pre-loaded NLTK resources\n",
    "\n",
    "# Load NLTK resources necessary for the tasks\n",
    "nltk.download('punkt', download_dir=r'C:\\Users\\nickr\\OneDrive\\Υπολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Tokenizers\n",
    "nltk.download('stopwords', download_dir=r'C:\\Users\\nickr\\OneDrive\\Υπολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Stopwords\n",
    "nltk.download('wordnet', download_dir=r'C:\\Users\\nickr\\OneDrive\\Υπολογιστής\\Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\') # Lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                           full_text  \\\n",
       " 0  Many people have car where they live. The thin...   \n",
       " 1  I am a scientist at NASA that is discussing th...   \n",
       " 2  People always wish they had the same technolog...   \n",
       " 3  We all heard about Venus, the planet without a...   \n",
       " 4  Dear, State Senator\\n\\nThis is a letter to arg...   \n",
       " \n",
       "                                           clean_text  \n",
       " 0  many people car live thing know use car alot t...  \n",
       " 1  scientist nasa discussing face mar explaining ...  \n",
       " 2  people always wish technology seen movie best ...  \n",
       " 3  heard venus planet without almost oxygen earth...  \n",
       " 4  dear state senator letter argue favor keeping ...  ,\n",
       "                                            full_text  \\\n",
       " 0  Many people have car where they live. The thin...   \n",
       " 1  I am a scientist at NASA that is discussing th...   \n",
       " 2  People always wish they had the same technolog...   \n",
       " \n",
       "                                           clean_text  \n",
       " 0  many people car live thing know use car alot t...  \n",
       " 1  scientist nasa discussing face mar explaining ...  \n",
       " 2  people always wish technology seen movie best ...  )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub('[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to both train and test data\n",
    "train_data['clean_text'] = train_data['full_text'].apply(preprocess_text)\n",
    "test_data['clean_text'] = test_data['full_text'].apply(preprocess_text)\n",
    "\n",
    "# Display first few rows to verify preprocessing\n",
    "train_data[['full_text', 'clean_text']].head(), test_data[['full_text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering\n",
    "* Linguistic Features: Extract features that represent the quality of writing, such as sentence complexity, vocabulary richness, grammar correctness, and coherence. Tools like the Natural Language Toolkit (NLTK) or spaCy can be helpful.\n",
    "* Text Embeddings: Use embeddings like Word2Vec, GloVe, or fastText to capture semantic relationships between words. Sentence and paragraph embeddings (e.g., from BERT or Sentence-BERT) can capture contextual nuances.\n",
    "* Syntactic Features: Parse trees and dependency graphs can help understand the syntactic structures of sentences, potentially indicating more complex writing abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04548181,\n",
       "         0.79017261, 0.04975925, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.04778838, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.0321821 , 0.1117614 ,\n",
       "         0.        , 0.19593373, 0.03828935, 0.        , 0.08631919,\n",
       "         0.03679849, 0.        , 0.        , 0.078892  , 0.0273742 ,\n",
       "         0.        , 0.        , 0.        , 0.03337794, 0.06588664,\n",
       "         0.        , 0.        , 0.        , 0.04189591, 0.        ,\n",
       "         0.11405601, 0.04082721, 0.03086056, 0.        , 0.13427865,\n",
       "         0.32417327, 0.        , 0.        , 0.1876218 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.04384533, 0.04065578,\n",
       "         0.        , 0.15026438, 0.        , 0.        , 0.        ,\n",
       "         0.07776394, 0.        , 0.05145062, 0.        , 0.11402972,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.13661017, 0.10336992, 0.        ,\n",
       "         0.17038731, 0.        , 0.        , 0.        , 0.03322437,\n",
       "         0.        , 0.        , 0.04177855, 0.        , 0.09694338],\n",
       "        [0.        , 0.        , 0.        , 0.12031148, 0.        ,\n",
       "         0.06052403, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.08259009, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.13234917, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.38101442,\n",
       "         0.        , 0.        , 0.06502235, 0.0445639 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10085184, 0.        ,\n",
       "         0.10191284, 0.54652212, 0.        , 0.10924507, 0.1516248 ,\n",
       "         0.13987506, 0.        , 0.062744  , 0.04621983, 0.091236  ,\n",
       "         0.39450627, 0.        , 0.        , 0.        , 0.36208903,\n",
       "         0.        , 0.        , 0.17093564, 0.        , 0.03718826,\n",
       "         0.        , 0.        , 0.08350221, 0.        , 0.06603448,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.20270679, 0.07770461, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.06131636, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.07013054, 0.09458492, 0.09542712, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06056744, 0.        , 0.10863886, 0.        ],\n",
       "        [0.03389527, 0.17204791, 0.        , 0.04384889, 0.        ,\n",
       "         0.06617602, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69263231, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.06772703, 0.        , 0.        , 0.        , 0.11619127,\n",
       "         0.03865233, 0.24130051, 0.17688265, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14446337, 0.        ,\n",
       "         0.        , 0.03987428, 0.        , 0.12181368, 0.05640431,\n",
       "         0.        , 0.        , 0.        , 0.02756745, 0.16336503,\n",
       "         0.        , 0.        , 0.04107377, 0.        , 0.04144604,\n",
       "         0.        , 0.03662262, 0.        , 0.02526802, 0.14963401,\n",
       "         0.        , 0.16234906, 0.        , 0.        , 0.        ,\n",
       "         0.05756236, 0.30907322, 0.04672459, 0.        , 0.06099159,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.04743942, 0.        , 0.07807622, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.11684862, 0.03352117, 0.0575491 ,\n",
       "         0.03295229, 0.        , 0.06314252, 0.02752881, 0.        ,\n",
       "         0.32421573, 0.        , 0.        , 0.02608463, 0.0238461 ,\n",
       "         0.        , 0.        , 0.        , 0.06239279, 0.05030353,\n",
       "         0.03407168, 0.03311174, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.0914367 , 0.        ,\n",
       "         0.04599825, 0.05034357, 0.32843039, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.11305302, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10058534, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.03347165, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.11497108, 0.04542132,\n",
       "         0.        , 0.        , 0.        , 0.04151312, 0.05761741,\n",
       "         0.        , 0.        , 0.        , 0.1053812 , 0.03466967,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.11007504,\n",
       "         0.        , 0.0429667 , 0.        , 0.        , 0.02826307,\n",
       "         0.05686019, 0.        , 0.        , 0.        , 0.45167574,\n",
       "         0.        , 0.        , 0.        , 0.04614299, 0.0427863 ,\n",
       "         0.        , 0.        , 0.        , 0.17716646, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.04580957, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1065984 , 0.03594226, 0.        , 0.        ,\n",
       "         0.        , 0.73031196, 0.        , 0.        , 0.0699309 ,\n",
       "         0.        , 0.        , 0.        , 0.11008736, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.03359203,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.56157192, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.099039  ,\n",
       "         0.1009494 , 0.56797115, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.02474935, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.12517673, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02533476,\n",
       "         0.        , 0.03298508, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.02373303, 0.        , 0.02065316,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.19276898, 0.13886939, 0.        , 0.        , 0.12506386,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.17538706,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.46804938, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate linguistic features\n",
    "def linguistic_features(text):\n",
    "    sentences = text.split('.')\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence != \"\"]\n",
    "    \n",
    "    # Average sentence length\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Vocabulary richness: Type-Token Ratio (TTR)\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words) if words else 0\n",
    "    \n",
    "    return avg_sentence_length, ttr\n",
    "\n",
    "# Apply linguistic features calculation\n",
    "train_data['avg_sentence_length'], train_data['ttr'] = zip(*train_data['clean_text'].map(linguistic_features))\n",
    "test_data['avg_sentence_length'], test_data['ttr'] = zip(*test_data['clean_text'].map(linguistic_features))\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Limit number of features to 100 for simplicity\n",
    "\n",
    "# Fit and transform the 'clean_text' column to create TF-IDF features\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
    "tfidf_test = tfidf_vectorizer.transform(test_data['clean_text'])\n",
    "\n",
    "# Example: Convert first 5 TF-IDF features of train data to dense format and display\n",
    "tfidf_train_dense_example = tfidf_train.todense()[:5]\n",
    "\n",
    "tfidf_train_dense_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Selection and Ensemble Methods\n",
    "* Advanced NLP Models: Utilize state-of-the-art language models like BERT, GPT, or their variants (RoBERTa, DistilBERT, etc.) fine-tuned on the essay dataset.\n",
    "* Ensemble Methods: Combine predictions from multiple models to improve accuracy. Techniques like bagging, boosting, or stacking can be particularly effective, especially when combining models that capture different aspects of writing quality.\n",
    "* Custom Scoring Layers: For neural networks, consider designing custom layers or loss functions that directly optimize for the competition’s evaluation metric (Quadratic Weighted Kappa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.2\n",
      "Transformers version: 4.39.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5340]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased').to(device)  # Move model to the right device\n",
    "\n",
    "class BERTRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased').to(device)  # Ensure model components are on the same device\n",
    "        self.regressor = nn.Linear(768, 1)  # Assuming the output of BERT is 768-dimensional\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        return self.regressor(outputs.pooler_output)  # Using the pooled output which is a good summary of the textual input\n",
    "\n",
    "# Instantiate the model\n",
    "regressor_model = BERTRegressor().to(device)  # Move the entire model to the right device\n",
    "\n",
    "# Example forward pass\n",
    "inputs = tokenizer(\"Example text input for BERT\", return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}  # Move input data to the correct device\n",
    "\n",
    "score = regressor_model(inputs['input_ids'], inputs['attention_mask'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Input device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device:\", next(regressor_model.parameters()).device)\n",
    "print(\"Input device:\", inputs['input_ids'].device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 8  # Smaller batch size for more granular update\n",
    "EPOCHS = 5  # More epochs since we're now treating this as partly a classification task\n",
    "MAX_LEN = 128  # Reduced max length to speed up training\n",
    "LR = 3e-5  # Learning rate adjusted\n",
    "EPS = 1e-8  # Small epsilon value for AdamW optimizer for numerical stability\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# Data Preparation Functions\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    token_ids = []\n",
    "    attention_masks = []\n",
    "    targets = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text, score = row['full_text'], row['score']\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        token_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "        targets.append(score - 1)  # Shift scale from 1-6 to 0-5\n",
    "\n",
    "    token_ids = torch.cat(token_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    targets = torch.tensor(targets)\n",
    "\n",
    "    dataset = TensorDataset(token_ids, attention_masks, targets)\n",
    "    sampler = RandomSampler(dataset)\n",
    "    loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Model Definition\n",
    "class BERTRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.2)  # Added dropout for regularization\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, 6)  # Output size is 6 for scores 1-6\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = BERTRegressor().to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # Using CrossEntropyLoss for classification\n",
    "\n",
    "# Training Function\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(data_loader)):\n",
    "        input_ids, attention_mask, targets = [r.to(device) for r in batch]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler\n",
    "    )\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['full_text', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)  # This should include both 'full_text' and 'score'\n",
    "train_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
